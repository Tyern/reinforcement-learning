{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for arrays\n",
    "import gym # pull the environment\n",
    "import time # to get the time\n",
    "import math # needed for calculations\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers  #gymの画像保存\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1]Q関数を離散化して定義する関数　------------\n",
    "# 観測した状態を離散値にデジタル変換する\n",
    "def bins(clip_min, clip_max, num):\n",
    "    return np.linspace(clip_min, clip_max, num + 1)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各値を離散値に変換\n",
    "def digitize_state(observation):\n",
    "    cart_pos, cart_v, pole_angle, pole_v = observation\n",
    "    digitized = [\n",
    "        np.digitize(cart_pos, bins=bins(-2.4, 2.4, num_dizitized)),\n",
    "        np.digitize(cart_v, bins=bins(-3.0, 3.0, num_dizitized)),\n",
    "        np.digitize(pole_angle, bins=bins(-0.5, 0.5, num_dizitized)),\n",
    "        np.digitize(pole_v, bins=bins(-2.0, 2.0, num_dizitized))\n",
    "    ]\n",
    "    return sum([x * (num_dizitized**i) for i, x in enumerate(digitized)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2]行動a(t)を求める関数 -------------------------------------\n",
    "def get_action(next_state, episode):\n",
    "    #徐々に最適行動のみをとる、ε-greedy法\n",
    "    # CAUTION1: epsilon increase as the game go on\n",
    "    # CAUTION2: changing value in the function that has not been related in parameter -> look for global variable address \n",
    "    epsilon = 0.5 * (1 / (episode + 1))\n",
    "    if epsilon <= np.random.uniform(0, 1):\n",
    "        next_action = np.argmax(q_table[next_state])\n",
    "    else:\n",
    "        next_action = np.random.choice([0, 1])\n",
    "    return next_action\n",
    " \n",
    " \n",
    "# [3]Qテーブルを更新する関数 -------------------------------------\n",
    "def update_Qtable(q_table, state, action, reward, next_state):\n",
    "    next_Max_Q=max(q_table[next_state][0],q_table[next_state][1] )\n",
    "    # update q tab;ke on q learning function\n",
    "    q_table[state, action] = (1 - alpha) * q_table[state, action] +\\\n",
    "            alpha * (reward + gamma * next_Max_Q)\n",
    "   \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# the variable for observation discretization \n",
    "# ------------------------------------------------------------\n",
    "# 状態を6分割^（4変数）にデジタル変換してQ関数（表）を作成\n",
    "num_dizitized = 6  #分割数\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Environment definition\n",
    "# ------------------------------------------------------------\n",
    "env = gym.make('CartPole-v0')\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# The must initialized variables (book existed)\n",
    "# ------------------------------------------------------------\n",
    "# Qtable must satisfied Qtable[state][action] being defined\n",
    "# Qtable is randomly initailized\n",
    "num_episodes = 2000  #総試行回数\n",
    "q_table = np.random.uniform(\n",
    "    low=-1, high=1, size=(num_dizitized**4, env.action_space.n))\n",
    "\n",
    "gamma = 0.99 # discount factor\n",
    "alpha = 0.5 #learning rate\n",
    "# CAUTION: dont have to initialize epsilon because it is being auto handled inside get_action function\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# The variable to define when to stop learning\n",
    "# ------------------------------------------------------------\n",
    "# If the step continue to grow after max number of step -> cut off\n",
    "max_number_of_steps = 200  #1試行のstep数\n",
    "\n",
    "# To ensure the stability, we will average the last num_consecutive_iterations number of episode\n",
    "# and compare with goal_average_reward\n",
    "num_consecutive_iterations = 100 #学習完了評価に使用する平均試行回数\n",
    "\n",
    "# the average of this vector will be used to compare with goal average reward\n",
    "total_reward_vec = np.zeros(num_consecutive_iterations)  #各試行の報酬を格納\n",
    "\n",
    "# goal average to be expected to get\n",
    "goal_average_reward = max_number_of_steps - 5  #この報酬を超えると学習終了（中心への制御なし）\n",
    "\n",
    "# record the last position before done\n",
    "final_x = np.zeros((num_episodes, 1))  #学習後、各試行のt=200でのｘの位置を格納\n",
    "\n",
    "islearned = 0  #学習が終わったフラグ\n",
    "isrender = 0  #描画フラグ\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Save pickle weight file name\n",
    "save_pickle_folder = \"cartpole_weight\"\n",
    "os.makedirs(save_pickle_folder, exist_ok=True)\n",
    "save_pickle_name = f\"{max_number_of_steps}-step-qtable.pkl\"\n",
    "save_pickle_path = os.path.join(save_pickle_folder, save_pickle_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n",
      "learning completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m8/y6jc4w1s4cbb3830x0n6zygr0000gn/T/ipykernel_98606/1361143143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mislearned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Q learning algorithm implementation\n",
    "for espisode in range(num_episodes):\n",
    "\n",
    "    # initialize S which is the S0 state\n",
    "    S = env.reset()\n",
    "    discretized_S = digitize_state(S)\n",
    "    is_done = False\n",
    "    t = 0\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Loop for each step in the espisode\n",
    "    while t < max_number_of_steps and not is_done:\n",
    "        t += 1\n",
    "        # choose A from Q on epsilon greedy algorithm\n",
    "        A = get_action(discretized_S, espisode)\n",
    "\n",
    "        S_next, reward, is_done, _ = env.step(A)\n",
    "\n",
    "        # for automatically playing game after learning\n",
    "        if islearned:\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        if is_done:\n",
    "            if t < goal_average_reward:\n",
    "                reward = - max_number_of_steps  #こけたら罰則\n",
    "            else:\n",
    "                reward = 1  #立ったまま終了時は罰則はなし\n",
    "        else:\n",
    "            reward = 1  #各ステップで立ってたら報酬追加\n",
    "\n",
    "        discretized_S_next = digitize_state(S_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        update_Qtable(q_table, discretized_S, A, reward, discretized_S_next)\n",
    "\n",
    "        discretized_S = discretized_S_next\n",
    "\n",
    "        #終了時の処理\n",
    "        if is_done:\n",
    "            # The reward vector is stack from the ebd to start position\n",
    "            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  #報酬を記録\n",
    "\n",
    "            if islearned:\n",
    "                final_x[espisode, 0] = S_next[0]\n",
    "\n",
    "            break\n",
    "\n",
    "    if np.average(total_reward_vec) > goal_average_reward:\n",
    "        # Learning completed\n",
    "        if islearned:\n",
    "            break\n",
    "        \n",
    "        print(\"learning completed\")\n",
    "        islearned = 1\n",
    "\n",
    "if islearned:\n",
    "    with open(save_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(q_table, f)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86d8283a4e1c43fd987bae320a06211c4a01e5b22163768483afa21b5867eb45"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('connect_x')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
